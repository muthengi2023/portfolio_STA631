{
  "articles": [
    {
      "path": "about.html",
      "title": "Maureen Muthengi",
      "author": [],
      "contents": "\n\nAbout Me\nMy name is Maureen Muthengi, and I am a dynamic professional with a robust background in data science, public policy, and information technology. I am currently pursuing a Master of Science in Data Science and Analytics from Grand Valley State University, complementing my Bachelor of Science in Information Technology from Jomo Kenyatta University of Agriculture and Technology.\nProfessional Experience\nI have extensive experience at the United Nations High Commissioner for Refugees (UNHCR) from 2009 to 2023, where I honed my skills in data extraction, analysis, and visualization using tools like PowerBI, Tableau, R, SQL, and Python. I am adept at project management, stakeholder collaboration, and delivering actionable insights.\nKey Initiatives\nI lead the “Donate a Pad Initiative” in Kitui, Kenya, providing sanitary towels and mentorship to girls in primary and high schools.\nProfessional Development\nI hold the Google Data Analytics Professional Certificate and the 10Alytics Data and Strategy certificate. I am also a data analyst for the Citizen’s Climate Lobby and Vice President of the African Student Council at Grand Valley State University.\nContributions and Recognitions\nI have presented papers at international conferences, received recognition for my service to UNHCR, and served as a board member and keynote speaker. I am passionate about leveraging data to drive impactful change.\n\n\n\n",
      "last_modified": "2024-08-03T22:45:45-04:00"
    },
    {
      "path": "index.html",
      "title": "Maureen Muthengi",
      "author": [],
      "contents": "\n\n          \n          \n          Maureen Muthengi\n          \n          \n          About\n          Portfolio\n          STA631\n          ☰\n          \n          \n      \n        \n          \n            \n              \n            \n              Maureen Muthengi\n            \n            \n              \n                \n                    \n                      \n                        LinkedIn\n                      \n                    \n                  \n                                    \n                    \n                      \n                        GitHub\n                      \n                    \n                  \n                                    \n                    \n                      \n                        Email\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            \n            About Me\n            My name is Maureen Muthengi, and I am a dynamic\n            professional with a robust background in data science,\n            public policy, and information technology. I am currently\n            pursuing a Master of Science in Data Science and Analytics\n            from Grand Valley State University, complementing my\n            Bachelor of Science in Information Technology from Jomo\n            Kenyatta University of Agriculture and Technology.\n            Professional Experience\n            I have extensive experience at the United Nations High\n            Commissioner for Refugees (UNHCR) from 2009 to 2023, where I\n            honed my skills in data extraction, analysis, and\n            visualization using tools like PowerBI, Tableau, R, SQL, and\n            Python. I am adept at project management, stakeholder\n            collaboration, and delivering actionable insights.\n            Key Initiatives\n            I lead the “Donate a Pad Initiative” in Kitui, Kenya,\n            providing sanitary towels and mentorship to girls in primary\n            and high schools.\n            Professional Development\n            I hold the Google Data Analytics Professional Certificate\n            and the 10Alytics Data and Strategy certificate. I am also a\n            data analyst for the Citizen’s Climate Lobby and Vice\n            President of the African Student Council at Grand Valley\n            State University.\n            Contributions and Recognition\n            I have presented papers at international conferences,\n            received recognition for my service to UNHCR, and served as\n            a board member and keynote speaker. I am passionate about\n            leveraging data to drive impactful change.\n            \n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Maureen Muthengi\n            \n            \n              \n                \n                                    \n                    \n                      LinkedIn\n                    \n                  \n                                    \n                    \n                      GitHub\n                    \n                  \n                                    \n                    \n                      Email\n                    \n                  \n                                  \n              \n            \n            \n              \n              About Me\n              My name is Maureen Muthengi, and I am a dynamic\n              professional with a robust background in data science,\n              public policy, and information technology. I am currently\n              pursuing a Master of Science in Data Science and Analytics\n              from Grand Valley State University, complementing my\n              Bachelor of Science in Information Technology from Jomo\n              Kenyatta University of Agriculture and Technology.\n              Professional Experience\n              I have extensive experience at the United Nations High\n              Commissioner for Refugees (UNHCR) from 2009 to 2023, where\n              I honed my skills in data extraction, analysis, and\n              visualization using tools like PowerBI, Tableau, R, SQL,\n              and Python. I am adept at project management, stakeholder\n              collaboration, and delivering actionable insights.\n              Key Initiatives\n              I lead the “Donate a Pad Initiative” in Kitui, Kenya,\n              providing sanitary towels and mentorship to girls in\n              primary and high schools.\n              Professional Development\n              I hold the Google Data Analytics Professional\n              Certificate and the 10Alytics Data and Strategy\n              certificate. I am also a data analyst for the Citizen’s\n              Climate Lobby and Vice President of the African Student\n              Council at Grand Valley State University.\n              Contributions and Recognition\n              I have presented papers at international conferences,\n              received recognition for my service to UNHCR, and served\n              as a board member and keynote speaker. I am passionate\n              about leveraging data to drive impactful change.\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2024-08-03T22:45:47-04:00"
    },
    {
      "path": "portfolio.html",
      "title": "Logistic Regression Model for Instacart Market Basket Analysis",
      "author": [],
      "contents": "\nIntroduction\nInstacart Market Basket Analysis\nWhich products will an Instacart consumer purchase again (reorder)?\nThe Instacart Market Basket Analysis is a project that aims to predict which products a consumer is likely to purchase again during their next order. By analyzing historical purchase data from millions of orders, the goal is to uncover patterns and insights about consumer behavior. This analysis helps in understanding customer preferences, improving product recommendations, and optimizing inventory management. Key aspects include examining product reorder rates, identifying frequent product combinations, and leveraging machine learning models to make accurate predictions about future purchases.\nFor this analysis, I will use a logistic regression model to predict the likelihood of a product being reordered. Logistic regression is suitable for this task as it can handle binary outcomes (reordered or not reordered) and provides probabilities that help in making informed predictions. By predicting which products an Instacart consumer will purchase again, businesses can tailor marketing strategies, enhance customer satisfaction, and drive sales growth through personalized shopping experiences.\nData Preprocessing\nInspect the data and preprocess it as necessary.\n\nData Visualization\nHistogram of Add to Cart Order\nA histogram can provide insights into how the position of products in the cart (add_to_cart_order) is distributed, which could help understand if certain positions are more common.\n\nScatter plot of add_to_cart_order vs. reordered\nA scatter plot to visualize the relationship between Add to Cart Order vs. Reordered\n\nFeature Engineering\nCreated features relevant for predicting reorder. We will use add_to_cart_order as a feature to predict whether a product will be reordered (target variable).\nModel Training\nSplit the data into training and testing sets and then train a logistic regression model.\n\nInterpretation:\nIntercept\nEstimate: 0.6822981\nStd. Error: 0.0021356\nz value: 319.5\nP-value: <2e-16\nThe intercept represents the log-odds of the response variable (reordered) when the predictor variable (add_to_cart_order) is zero. A high z value and a p-value less than 2e-16 indicate that the intercept is highly statistically significant.\nadd_to_cart_order:\nEstimate: -0.0379922\nStd. Error: 0.0001941\nz value: -195.8\nP-value: 2e-16\nThe coefficient for add_to_cart_order represents the change in the log-odds of the response variable for a one-unit increase in add_to_cart_order. Since the estimate is -0.0379922, it means that for each additional unit increase in add_to_cart_order, the log-odds of a product being reordered decreases by -0.0379922.\nThe negative sign indicates an inverse relationship between add_to_cart_order and the likelihood of reordering. The high absolute value of the z score and the p-value of less than 2e-16 suggest that this predictor is highly statistically significant.\nA p-value of less than 0.001 indicates strong evidence against the null hypothesis, suggesting that both the intercept and add_to_cart_order significantly influence the probability of a product being reordered.\nThe logistic regression model indicates that the position of a product in the cart (add_to_cart_order) has a significant negative effect on the likelihood of reordering the product. As the add_to_cart_order increases, the probability of reordering the product decreases. The model is statistically significant with both predictors having very low p-values, and the overall fit of the model is reasonable based on the AIC and deviance values.\nModel Evaluation\n\nInterpretation\nAccuracy: 0.5973, this means that the model correctly predicts the reorder status 59.73% of the time. While this is better than random guessing, it is not particularly high, suggesting the model has room for improvement.\nThe model has high specificity, meaning it is good at identifying non-reordered products, but low sensitivity, meaning it struggles to identify reordered products.\nThe overall accuracy is modest at 59.73%, with a balanced accuracy of 52.66%, indicating the model performs moderately well but has significant room for improvement.\nThe model’s predictions are slightly better than random guessing.\nDescribe probability as a foundation of statistical modeling, including inference and maximum likelihood estimation\nProbability forms the bedrock of statistical modeling, providing the framework for quantifying uncertainty and making inferences about a population based on sample data. Here’s how probability underpins statistical modeling, including inference and maximum likelihood estimation:\nProbability in Statistical Modeling\nFoundation for Models: Defines distributions and random variables.\nQuantifying Uncertainty: Helps assess data variability and model fit.\nRandom Variables: These are variables whose values are outcomes of a random phenomenon. Probability theory helps in defining and analyzing these variables and their distributions, which is crucial for creating statistical models.\nInference\nParameter Estimation: Uses sample data to estimate population parameters.\nHypothesis Testing: Evaluates evidence against a null hypothesis using p-values.\nConfidence Intervals: Provides a range of values for parameter estimates.\nMaximum Likelihood Estimation (MLE)\nLikelihood Function: Represents the probability of observed data as a function of parameters.\nMaximizing Likelihood: Finds parameter values that make the observed data most probable.\nProcess: Define, maximize, and evaluate the likelihood function to estimate parameters.\nApplying Probability and MLE to the Instacart Project\nIn the Instacart Market Basket Analysis, you are predicting whether a product will be reordered based on features like add_to_cart_order.\nHere’s how probability and MLE are foundational to this process:\nModel Specification: You specify a logistic regression model to predict the probability of reordering.\nLikelihood Function: You derive the likelihood function based on the logistic regression model.\nParameter Estimation: Using MLE, you estimate the model parameters (coefficients) that maximize the likelihood of observing the given data.\nInference: You use statistical inference to make predictions and assess the significance of the predictors.\nModel Evaluation: You evaluate the model’s performance using metrics like accuracy, precision, recall, and AUC-ROC, which are all rooted in probability.\nBy understanding and applying these concepts, you can build robust statistical models that provide valuable insights into customer behavior and improve decision-making processes in the Instacart Market Basket Analysis.\nStep-by-Step Model Specification for Logistic Regression\nspecifying a logistic regression model to predict the probability of reordering in the Instacart Market Basket Analysis.\nDefine the Response Variable\nThe response variable in our logistic regression model is binary, indicating whether a product is reordered (reordered = 1) or not (reordered = 0).\nDefine the Predictor Variables\nThe predictor variables are the features that we use to predict the response variable. For simplicity, let’s start with a single predictor: add_to_cart_order.\nSpecify the Logistic Regression Model\nEvaluating the Model Fit and Making Predictions\nNow that we have specified the logistic regression model, let’s move on to evaluating the model fit and making predictions.\nIn logistic regression, we model the log-odds of the response variable as a linear combination of the predictor variables. The logistic regression model can be written as:\nlog(𝑃(𝑦=1)/1−𝑃(𝑦=1))=𝛽0+𝛽1𝑋\nWhere:\nP(y=1) is the probability of reordering.\n𝛽0 is the intercept.\nβ 1 is the coefficient for the predictor variable 𝑋 (add_to_cart_order).\n\nInterpretation\nCoefficients\nIntercept (β₀): 0.6823680, this is the log-odds of reordering when add_to_cart_order is 0. In probability terms, this translates to the odds of reordering when the add_to_cart_order is at its baseline (which might be a low value if the minimum value of add_to_cart_order is not zero).\nadd_to_cart_order (β₁): -0.0379998, for each one-unit increase in add_to_cart_order, the log-odds of reordering decreases by 0.0380813. This suggests that as a product is placed further down the cart, the probability of reordering decreases.\nSignificance: The p-value is very small (< 2e-16), indicating that add_to_cart_order is highly significant in predicting the likelihood of reordering.\nModel Fit Statistics\nNull Deviance: 43914253, the deviance of the model with no predictors (only the intercept). It measures the goodness of fit of a model that only includes the intercept.\nResidual Deviance: 43342897, the deviance of the model with predictors. It shows how well the model with the predictors fits compared to the null model. Lower residual deviance indicates a better fit.\nDegrees of Freedom:\nAIC (Akaike Information Criterion): 43342901, AIC is used to compare models; lower AIC values indicate a better-fitting model. It takes into account the goodness of fit and the complexity of the model.\nNumber of Fisher Scoring Iterations: 4 this is the number of iterations Fisher’s scoring algorithm took to converge to a solution.\nThis model suggests that add_to_cart_order is a significant predictor of the probability of reordering, with higher cart positions being associated with a lower likelihood of reordering.\nGoodness of Fit: The residual deviance is close to the null deviance, suggesting the model explains some of the variability in the data, but there may still be room for improvement.\nModel Selection: The AIC can be used to compare this model to other models (if you have them) to determine which model has the best trade-off between fit and complexity.\nMaking Predictions\nWe can use the fitted model to predict the probability of reordering for the given data. We’ll use the predict function to get the predicted probabilities.\n\nInterpretation\nadd_to_cart_order: This shows the position of the product in the cart. For example, in the first row, add_to_cart_order is 1, meaning this product was the first item added to the cart.\nreordered: This binary indicator shows whether the product was reordered (1) or not (0). In the example, the first product was reordered (reordered = 1), while the third product was not (reordered = 0).\npredicted_prob: This column likely represents the predicted probability of a product being reordered, as predicted by a model. For instance, in the first row, predicted_prob is 0.6562661, suggesting a high probability of the product being reordered. Helps to gauge how likely each product is to be reordered according to the model’s predictions. For example, in the first row, predicted_prob is 0.6562661, suggesting a high probability of the product being reordered.f\nAssess Model Performance\nTo assess the model performance, we will calculate the accuracy and confusion matrix.\n\nInterpretation\nAccuracy: 0.5972, this is the proportion of all correct predictions (both 0 and 1) out of the total number of predictions. It indicates that about 59.67% of the predictions made by the model are correct.\n95% Confidence Interval for Accuracy: (0.597, 0.5973), we are 95% confident that the true accuracy of the model is between 59.7% and 59.73%.\nConducting Model Selection for Logistic Regression\nTo improve model performance, we can conduct model selection by comparing a set of candidate models with different predictor variables. We’ll use techniques such as Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to compare models and select the best one.\n1 Define Candidate Models\nWe’ll define a few candidate models with different combinations of predictor variables. For simplicity, let’s start with the following:\nModel 1: reordered ~ add_to_cart_order\nModel 2: reordered ~ add_to_cart_order + order_id\nModel 3: reordered ~ add_to_cart_order + product_id\nModel 4: reordered ~ add_to_cart_order + order_id + product_id\nResults\nModel 1: reordered ~ add_to_cart_order\nAIC: 4,335,468\nBIC: 4,335,494\nModel 2: reordered ~ add_to_cart_order + order_id\nAIC: 4,335,470\nBIC: 4,335,509\nModel 3: reordered ~ add_to_cart_order + product_id\nAIC: 4,335,389\nBIC: 4,335,428\nModel 4: reordered ~ add_to_cart_order + order_id + product_id\nAIC: 4,335,390\nBIC: 4,335,442\n2 Compare the Models\nWe’ll fit each model using the glm function.\nInterpretation\nBest Model Based on AIC\nModel 3 (reordered ~ add_to_cart_order + product_id) has the lowest AIC value (4,335,389), indicating it is the best model in terms of balancing goodness of fit and model complexity based on AIC.\nBest Model Based on BIC\nModel 3 (reordered ~ add_to_cart_order + product_id) also has the lowest BIC value (4,335,428), indicating it is the best model in terms of balancing fit and complexity based on BIC.\n4 Select the Best Model based on AIC\nWe’ll choose the model with the lowest AIC/BIC.\nModel 3 (reordered ~ add_to_cart_order + product_id) is the preferred model according to both AIC and BIC, as it has the lowest values for both criteria. This model provides a good trade-off between fit and complexity, suggesting that it is the most appropriate model among the candidates for predicting the probability of reordering.\nCommunicate the results of statistical models to a general audience\nWorking on a model to predict whether a customer will reorder a product on Instacart based on their shopping behavior. The goal is to improve our understanding of what influences reordering and make better recommendations to enhance customer satisfaction.\nKey Findings\nPredictor Importance:\nTested different factors to see what best predicts if a product will be reordered. Among these factors, the position of the product in the cart (add_to_cart_order) and the product itself (product_id) were found to be important.\nBest Model:\nAfter evaluating several models, the most effective one used both the position of the product in the cart and the specific product details to predict reordering. This model performed better than others in terms of accuracy and reliability.\nModel Performance:\nAccuracy: Our best model correctly predicted whether a product would be reordered about 60% of the time. This means that in 6 out of 10 cases, the model’s predictions were accurate.\nPredicting Reorders: The model is good at identifying products that are not reordered, with a high success rate in these predictions. However, it’s less effective at spotting products that will be reordered.\nConclusion\nBased on the analysis results where product_id and add_to_cart_order are identified as significant predictors of reordered, we can draw some conclusions about Instacart Market Basket Analysis:\nReordering Patterns: The analysis reveals that certain products (product_id) and their position in the cart (add_to_cart_order) significantly influence whether a consumer will reorder them. For instance, products with higher reorder rates (reorder_rate closer to 1) tend to show patterns where they are frequently placed early in the cart (add_to_cart_order 1 or 2), suggesting they are prioritized by consumers.\nPredictive Insights: Products that are frequently reordered (reorder_rate high) often exhibit specific characteristics. These might include popular products (product_id) that are regularly placed at the beginning of the shopping cart (add_to_cart_order low values). This behavior indicates that certain products are more likely to be repurchased, possibly due to consumer preferences or daily necessities.\nThese insights help in understanding consumer behavior and can guide strategies to optimize product placements, marketing efforts, and inventory management on platforms like Instacart, enhancing customer satisfaction and retention.\nImproved Recommendations: By using this model, we can tailor product recommendations more effectively. For example, if we know that a product is often reordered when placed in a specific position in the cart, we can highlight such products in our recommendations.\nFocus on Popular Products: The model also helps identify which products are likely to be reordered based on their specific characteristics. This allows us to focus on popular items and ensure they are available when needed.\nEnhanced Customer Experience: With better predictions, we can enhance the shopping experience by offering products that customers are more likely to reorder, leading to higher customer satisfaction and loyalty.\nRecommendations\nInstacart could use these insights to optimize their recommendations and promotions. Products identified as likely to be reordered could be featured more prominently or suggested during checkout to increase reordering rates.\nConsumer Behavior Understanding: This analysis provides a deeper understanding of consumer behavior on Instacart, helping to tailor marketing strategies and product placements that enhance customer satisfaction and loyalty.\nIn essence, Instacart can leverage these findings to predict which products consumers are likely to purchase again, enhancing their market basket analysis and overall service optimization.\nReflections\nThis course was highly interactive, led by Professor Bradford Dykes, who conducted sessions every Monday. We engaged in activities, assignments, and projects that allowed us to apply the knowledge acquired on various models taught in class. I appreciated the use of the “Muddy” platform in Ms Teams, where we could share our challenges, errors, and any issues encountered while working on the activities. This facilitated discussions with classmates, enabling us to find solutions collaboratively. The one-on-one sessions with Professor Dykes were immensely helpful whenever we faced challenges with the activities. Whenever I faced a challenge in class or when working on the activities, I asked questions in class, had separate meetings with Professor who clarified on the questions I had. I appreciated we had presentations and were able to get more insights from my classmates projects. Additionally, I valued the requirement to share our assignments, projects, and activities with peers for recommendations before submission, as their feedback provided valuable insights and enhancements to our work.\n\n\n\n",
      "last_modified": "2024-08-03T22:45:48-04:00"
    },
    {
      "path": "STA631.html",
      "title": "STA 631: Statistical Modeling and Regression",
      "author": [],
      "contents": "\nThis course follows a traditional and modern computationally intensive statistical modeling techniques. Basics of probability theory, including conditional probability, Bayes’ Theorem, and univariate probability models. Regression modeling and prediction including simple linear, multiple, logistic, Poisson, nonlinear and nonparametric regression. Methods for model selection and shrinkage. Emphasis is on application and interpretation using statistical software.\nTopics Covered\nSimple Linear Regression\nSimple Linear Regression is a statistical method used to model the relationship between a dependent variable and a single independent variable. The model assumes a linear relationship, where the equation of the line (y = mx + c) represents the best fit for the data points. This method is used to predict the value of the dependent variable based on the independent variable.\nMultiple Linear Regression\nMultiple Linear Regression extends simple linear regression by using two or more independent variables to predict a dependent variable. The model still assumes a linear relationship but includes additional predictors to improve the accuracy of the prediction. The equation takes the form y = b0 + b1x1 + b2x2 + … + bnxn.\nLogistic Regression\nLogistic Regression is used when the dependent variable is categorical, often binary. It models the probability that a given input point belongs to a certain class. Instead of fitting a line, it fits an S-shaped logistic function, which outputs probabilities that can be mapped to binary outcomes.\nMultinomial Regression\nMultinomial Regression is an extension of logistic regression that deals with dependent variables with more than two categories. It models the probabilities of the different possible outcomes of a categorical dependent variable, given a set of independent variables.\nGeneralized Linear Model (GLM)\nGeneralized Linear Models extend linear regression to models with a non-normal distribution of the dependent variable. GLMs consist of three components: a linear predictor, a link function, and a variance function that describes the distribution of the dependent variable. Examples include logistic regression and Poisson regression.\nResampling and Cross-validation\nResampling methods, such as bootstrapping and permutation tests, involve repeatedly drawing samples from the data and refitting the model to understand the variability of the estimator. Cross-validation is a technique for assessing how well a model generalizes to an independent dataset by partitioning the data into training and testing sets multiple times.\nModel Selection and Multiple Testing\nModel selection involves choosing the best model from a set of potential models based on criteria like AIC, BIC, or adjusted R-squared. Multiple testing refers to the statistical analysis process where multiple hypotheses are tested simultaneously. Adjustments like Bonferroni correction are used to control the family-wise error rate.\nTools Used\nR Studio\nRStudio is our go-to integrated development environment (IDE) for R, the programming language we use for statistical computing and graphics in STA 631. It offers a user-friendly interface with a console, syntax-highlighting editor, and essential tools for plotting, history, and workspace management. This setup makes developing our R scripts and projects both efficient and enjoyable.\nGitHub\nGitHub is an essential tool for us in STA 631, facilitating version control and collaborative software development. This web-based platform hosts our Git repositories and provides us with tools for code review, project management, and collaboration. By using GitHub, we can efficiently track changes, manage issues, and contribute to projects, making it an invaluable resource for our class activities and projects.\nText Materials\nAn Introduction to Statistical Learning with R (ISLR)\n“An Introduction to Statistical Learning with R” (ISLR) by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani is a comprehensive guide to statistical learning methods. It covers a range of topics, including linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more, with practical examples using R.\nData Feminism\n“Data Feminism” by Catherine D’Ignazio and Lauren F. Klein explores the intersection of data science and feminism. The book discusses how data science can be used to challenge power structures and promote social justice. It emphasizes the importance of considering context, ethics, and the voices of marginalized groups in data practices.\nTidy Modeling with R\n“Tidy Modeling with R” by Max Kuhn and Julia Silge introduces the tidymodels framework for modeling and machine learning in R. The book covers a range of modeling techniques, from basic to advanced, using a consistent and tidy approach. It emphasizes practical applications, reproducibility, and ease of use.\nR for Data Science (2nd Edition)\n“R for Data Science” by Hadley Wickham and Garrett Grolemund is an essential guide for anyone working with data in R. The second edition covers the tidyverse, a collection of R packages designed for data science. Topics include data import, tidying, transformation, visualization, and modeling, with a focus on practical applications and real-world examples.\n\n\n\n",
      "last_modified": "2024-08-03T22:45:48-04:00"
    }
  ],
  "collections": []
}
